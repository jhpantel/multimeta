View(data_use)
mod_linear <- lm(dp_m_err ~ dp_N + sv_N)
# 4. Fit non-linear growth model to data ----------------------------------
dat_use <- rbind(data[,,1],data[,,2])
mod_start <- m5_m0 ~ (L*exp(-A*Daphnia_pulex - B*Simocephalus))^5
mod_linear <- lm(dp_m_err ~ dp_N + sv_N, data=dat_use)
View(dat_use)
dat_use <- as.data.frame(dat_use)
mod_linear <- lm(dp_m_err ~ dp_N + sv_N, data=dat_use)
summary(mod_use)
summary(mod_linear)
coef(mod_linear)
abs(coef(mod_linear))
abs(coef(mod_linear))[2] / abs(coef(mod_linear))[1]
abs(coef(mod_linear))[3] / abs(coef(mod_linear))[1]
res <- nls(mod_start,data=dat_use,start=list(L=coef(mod_linear)[1],A=abs(coef(mod_linear))[2] / abs(coef(mod_linear))[1],B=abs(coef(mod_linear))[3] / abs(coef(mod_linear))[1]))
mod_start <- dp_m_err ~ (L*exp(-A*Daphnia_pulex - B*Simocephalus))^5
res <- nls(mod_start,data=dat_use,start=list(L=coef(mod_linear)[1],A=abs(coef(mod_linear))[2] / abs(coef(mod_linear))[1],B=abs(coef(mod_linear))[3] / abs(coef(mod_linear))[1]))
mod_start <- dp_m_err ~ (L*exp(-A*dp_N - B*sv_N))^5
res <- nls(mod_start,data=dat_use,start=list(L=coef(mod_linear)[1],A=abs(coef(mod_linear))[2] / abs(coef(mod_linear))[1],B=abs(coef(mod_linear))[3] / abs(coef(mod_linear))[1]))
summary(res)
aic(res)
AIC(res)
extractAIC(res)
AIC(res)
q()
library(MASS)
?lda
?rda
library(vegan)
?rda
?lda
Iris <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
Sp = rep(c("s","c","v"), rep(50,3)))
train <- sample(1:150, 75)
View(Iris)
z <- lda(Sp ~ ., Iris)
plot(z)
?formula
q()
## intrinsic rate of increase
dp_lambda <- 1.4
sv_lambda <- 1.3
## intraspecific competition coefficient
dp_alpha_ii <- 0.05
sv_alpha_ii <- 0.05
## interspecific competition coefficient
dp_alpha_ij <- 0.015
sv_alpha_ij <- 0.025
## initial length (day 0), micrometers
dp_m0 <- 0.8
sv_m0 <- 0.5
ricker <- function(t,dp_N,sv_N,dp_lambda,sv_lambda,dp_alpha_ii,sv_alpha_ii,dp_alpha_ij,sv_alpha_ij,dp_m0,sv_m0){
dp_m5 <- dp_m0*((dp_lambda*exp(-dp_alpha_ii*dp_N - dp_alpha_ij*sv_N))^t)
sv_m5 <- sv_m0*((sv_lambda*exp(-sv_alpha_ii*sv_N - sv_alpha_ij*dp_N))^t)
m5 <- list(dp_m5=dp_m5,sv_m5=sv_m5)
return(m5)
}
dat <- array(NA,dim=c(10,7),dimnames=list(NULL,c("trt","dp_N","sv_N","dp_m0","sv_m0","dp_m5","sv_m5")))
dat <- as.data.frame(dat)
dat$trt <- 1:10
dat$dp_N <- c(1,0,5,3,2,0,10,6,4,0)
dat$sv_N <- c(0,1,0,2,3,5,0,4,6,10)
dat$dp_m0 <- dp_m0
dat$sv_m0 <- sv_m0
View(dat)
for(i in 1:nrow(dat)){
xx <- ricker(5,dat$dp_N[i],dat$sv_N[i],dp_lambda,sv_lambda,dp_alpha_ii,sv_alpha_ii,dp_alpha_ij,sv_alpha_ij,dat$dp_m0[i],dat$sv_m0[i])
dat$dp_m5[i] <- xx$dp_m5
dat$sv_m5[i] <- xx$sv_m5
}
# Eliminate growth values where a species wasn't present and add random noise to mt / m0
dat$dp_m5[dat$dp_N == 0] <- NA
dat$sv_m5[dat$sv_N == 0] <- NA
dat$dp_m_err <- NA
dat$sv_m_err <- NA
dat$dp_m_err[is.finite(dat$dp_m5)] <- dat$dp_m5[is.finite(dat$dp_m5)] / dat$dp_m0[is.finite(dat$dp_m5)] + rnorm(length(dat$dp_m5[is.finite(dat$dp_m5)]),0,.01)
dat$sv_m_err[is.finite(dat$sv_m5)] <- dat$sv_m5[is.finite(dat$sv_m5)] / dat$sv_m0[is.finite(dat$sv_m5)] + rnorm(length(dat$sv_m5[is.finite(dat$sv_m5)]),0,.01)
data <- array(dim=c(10,9,2),dimnames=list(NULL,colnames(dat),c("A","B")))
data[,,1] <- as.matrix(dat)
## Repeat data generation for Replicate B
dat <- array(NA,dim=c(10,7),dimnames=list(NULL,c("trt","dp_N","sv_N","dp_m0","sv_m0","dp_m5","sv_m5")))
dat <- as.data.frame(dat)
dat$trt <- 1:10
dat$dp_N <- c(1,0,5,3,2,0,10,6,4,0)
dat$sv_N <- c(0,1,0,2,3,5,0,4,6,10)
dat$dp_m0 <- dp_m0
dat$sv_m0 <- sv_m0
for(i in 1:nrow(dat)){
xx <- ricker(5,dat$dp_N[i],dat$sv_N[i],dp_lambda,sv_lambda,dp_alpha_ii,sv_alpha_ii,dp_alpha_ij,sv_alpha_ij,dat$dp_m0[i],dat$sv_m0[i])
dat$dp_m5[i] <- xx$dp_m5
dat$sv_m5[i] <- xx$sv_m5
}
dat$dp_m5[dat$dp_N == 0] <- NA
dat$sv_m5[dat$sv_N == 0] <- NA
dat$dp_m_err <- NA
dat$sv_m_err <- NA
dat$dp_m_err[is.finite(dat$dp_m5)] <- dat$dp_m5[is.finite(dat$dp_m5)] / dat$dp_m0[is.finite(dat$dp_m5)] + rnorm(length(dat$dp_m5[is.finite(dat$dp_m5)]),0,.01)
dat$sv_m_err[is.finite(dat$sv_m5)] <- dat$sv_m5[is.finite(dat$sv_m5)] / dat$sv_m0[is.finite(dat$sv_m5)] + rnorm(length(dat$sv_m5[is.finite(dat$sv_m5)]),0,.01)
data[,,2] <- as.matrix(dat)
data
# 4. Fit non-linear growth model to data ----------------------------------
dat_use <- rbind(data[,,1],data[,,2])
dat_use <- as.data.frame(dat_use)
View(dat_use)
mod_start <- dp_m_err ~ (L*exp(-A*dp_N - B*sv_N))^5
mod_linear <- lm(dp_m_err ~ dp_N + sv_N, data=dat_use)
summary(mod_linear)
res <- nls(mod_start,data=dat_use,start=list(L=coef(mod_linear)[1],A=abs(coef(mod_linear))[2] / abs(coef(mod_linear))[1],B=abs(coef(mod_linear))[3] / abs(coef(mod_linear))[1]))
res
summary(res)
# S. vetulus
mod_start_sv <- sv_m_err ~ (L*exp(-A*sv_N - B*dp_N))^5
mod_linear_sv <- lm(sv_m_err ~ sv_N + dp_N, data=dat_use)
summary(mod_linear_sv)
res_sv <- nls(mod_start_sv,data=dat_use,start=list(L=coef(mod_linear_sv)[1],A=abs(coef(mod_linear_sv))[2] / abs(coef(mod_linear_sv))[1],B=abs(coef(mod_linear_sv))[3] / abs(coef(mod_linear_sv))[1]))
summary(res_sv)
AIC(res)
q()
###############Section 1
#(1)
co2up <- CO2$uptake		#Assign the contents of the variable 'uptake' to the  new vector 'co2up'
co2up
#(2)
cat('min: ', min(co2up), "\n")
cat('max: ', max(co2up), "\n")
cat('median: ', median(co2up), "\n")
cat('mean: ', mean(co2up), "\n")
cat('variance: ', var(co2up), "\n")
cat('standard deviation: ', sd(co2up), "\n", "\n")
#(3)
out <- function(x){
a <- sum(x)/length(x)	#Get the average of all values
b <- (co2up - a)^2					#Squared deviations from mean
c <- sqrt((sum(b))/(length(x)-1)) #sum squared deviations, divide by sample N-1, take square root
d <- c/a  #coefficient of variation
return(list(st_dev=c,CV=d))
}
result <- out(co2up)
result
cat('Standard Deviation calculated by Jelena:', result$st_dev, "\n", "\n")  #Displays standard deviation
cat('Coefficient of Variation calculated by Jelena:', result$CV, "\n", "\n")  #Displays coefficient of variation
#(4)
co2up.int <- as.integer(co2up)	#Converts co2up to integer values, saves into new variable
co2up.int
#To calculate and display the mode:
# OPTION 1
rec <- rep(NA,length(unique(co2up.int)))
ord <- sort(unique(co2up.int))
for(i in 1:length(unique(co2up.int))){
rec[i] <- length(which(co2up.int == ord[i]))
}
mode <- ord[rec == 5]
cat('mode: ', mode, "\n")
# OPTION 2
m <- table(co2up.int)	#Generates a list with the values of CO2 ranked by frequency of appearance
mode <- as.numeric(names(m)[m==max(m)]) #gives the names of the table headings with the highest frequency of occurence
cat('mode: ', mode, "\n")
#(5)
q <- c(F,T)						#Sets up a vector with the 1st value as false, 2nd value as true
r <- -(co2up[q])				#Gives a subset of co2up, where each true value (every other value) is returned and switched to a negative value
co2up.t <- replace(co2up, q, r)		#Replaces every value of co2up that is true (every other value) with the value found in n, which is the negative of that value
co2up.t
#(6)
co2up.ln <- log(co2up)			#Calculates the natural log of co2up
co2up.ln
co2up.ln.t <- log(co2up.t)		#Every other value is 'NaN', which means 'Not A Number.'  Natural log of negative values are undefined.
co2up.ln.t
#To fix this, I want to only calculate the natural log of the positive values.
co2up.t.ln.sub <- co2up.ln.t[co2up.ln.t != "NaN"]     #This returns the values of the vector which exist
co2up.t.ln.sub
#(7)
attach(CO2)		#Attaches the CO2 dataset to the workspace
co2up.pearson <- cor(conc, uptake)		#Calculates the Pearson correlation
co2up.pearson
#(8)
mean_MS <- mean(uptake[Type == "Mississippi"])
mean_Q <- mean(uptake[Type == "Quebec"])
cat('mean of Mississippi plant uptake values: ', mean_MS, "\n", "\n")	#Calculates the mean uptake of Mississippi plants
cat('mean of Quebec plant uptake values: ', mean_Q, "\n", "\n")		#Calculates the mean uptake of Quebec plants
#(9)
MS <- uptake[Type == "Mississippi"]	#The uptake values for Mississippi plants
Q <- uptake[Type == "Quebec"]	#The uptake values for Quebec plants
hist(MS, col = "darkorange3", breaks = seq(6, 47, 1), ylim = range(0:5), main = "Histogram of CO2 uptake values", xlab = "uptake")		#Generates a histogram with uptake values from Mississippi plants in burnt orange
hist(Q, col = "white", add = T, breaks = seq(6, 47, 1), ylim = range(0:5), main = "Histogram of CO2 uptake values", xlab = "uptake")		#Adds uptake values from Quebec plants in white into existing histogram
detach(CO2)
#(10a)
pmf_binom <- function(X,n,p){
(factorial(n)/(factorial(X)*factorial(n-X)))*(p^X)*((1-p)^(n-X))
}
#(10b)
moment_binom <- function(n,p){
mean <- n*p
var <- n*p*(1-p)
skew <- (1-2*p)/sqrt(n*p*(1-p))
kurt <- (1-6*p*(1-p))/(n*p*(1-p))
return(list(mean=mean,var=var,skew=skew,kurt=kurt))
}
#(10c)
set1 <- rep(NA,21)
for(i in 0:20){
set1[i+1] <- pmf_binom(i,20,0.5)
}
set2 <- rep(NA,21)
for(i in 0:20){
set2[i+1] <- pmf_binom(i,20,0.7)
}
set3 <- rep(NA,41)
for(i in 0:40){
set3[i+1] <- pmf_binom(i,40,0.5)
}
plot(set1,pch=19,col="black",ylim=c(0,max(c(set1,set2,set3))),xlim=c(0,41))
points(set2,pch=19,col="skyblue")
points(set3,pch=19,col="gray")
#(10d)
set1 <- rep(NA,21)
for(i in 0:20){
set1[i+1] <- pbinom(i,20,0.5)
}
set2 <- rep(NA,21)
for(i in 0:20){
set2[i+1] <- pbinom(i,20,0.7)
}
set3 <- rep(NA,41)
for(i in 0:40){
set3[i+1] <- pbinom(i,40,0.5)
}
plot(set1,pch=19,col="black",ylim=c(0,1),xlim=c(0,41))
points(set2,pch=19,col="skyblue")
points(set3,pch=19,col="gray")
#(10e)
cbind(moment_binom(20,.5),moment_binom(20,.7),moment_binom(40,.5))
#(10f)
plot(set1,pch=19,col="black",ylim=c(0,1),xlim=c(0,41))
points(set2,pch=19,col="skyblue")
#(10f)
plot(set1,pch=19,col="black",ylim=c(0,max(c(set1,set2,set3))),xlim=c(0,41))
points(set2,pch=19,col="skyblue")
#(10c)
set1 <- rep(NA,21)
for(i in 0:20){
set1[i+1] <- pmf_binom(i,20,0.5)
}
set2 <- rep(NA,21)
for(i in 0:20){
set2[i+1] <- pmf_binom(i,20,0.7)
}
set3 <- rep(NA,41)
for(i in 0:40){
set3[i+1] <- pmf_binom(i,40,0.5)
}
#(10f)
plot(set1,pch=19,col="black",ylim=c(0,max(c(set1,set2,set3))),xlim=c(0,41))
points(set2,pch=19,col="skyblue")
set1 <- rep(NA,21)
for(i in 0:20){
set1[i+1] <- pmf_binom(i,20,0.5)
}
set2 <- rep(NA,21)
for(i in 0:20){
set2[i+1] <- pmf_binom(i,20,0.7)
}
plot(set1,pch=19,col="black",ylim=c(0,max(c(set1,set2,set3))),xlim=c(0,41))
points(set2,pch=19,col="skyblue")
plot(set1,pch=19,col="black",ylim=c(0,max(c(set1,set2))),xlim=c(0,41))
points(set2,pch=19,col="skyblue")
plot(set1,pch=19,col="black",ylim=c(0,max(c(set1,set2))),xlim=c(0,25))
points(set2,pch=19,col="skyblue")
cbind(moment_binom(20,.5),moment_binom(20,.7))
set2
set3 <- rep(NA,41)
for(i in 0:40){
set3[i+1] <- pmf_binom(i,40,0.5)
}
plot(set1,pch=19,col="black",ylim=c(0,max(c(set1,set2,set3))),xlim=c(0,41))
points(set3,pch=19,col="gray")
cbind(moment_binom(20,.5),moment_binom(40,.5))
#(11a)
dpois(4,0.75)
#(11b)
dpois(0,0.75)
# pmf
set1 <- rep(NA,21)
for(i in 0:20){
set1[i+1] <- dpois(i,1)
}
set2 <- rep(NA,21)
for(i in 0:20){
set2[i+1] <- dpois(i,4)
}
set3 <- rep(NA,41)
for(i in 0:40){
set3[i+1] <-dpois(i,10)
}
plot(set1,pch=19,col="black",ylim=c(0,max(c(set1,set2,set3))),xlim=c(0,21))
points(set2,pch=19,col="skyblue")
points(set3,pch=19,col="gray")
#cumulative
set1 <- rep(NA,21)
for(i in 0:20){
set1[i+1] <- ppois(i,1)
}
set2 <- rep(NA,21)
for(i in 0:20){
set2[i+1] <- ppois(i,4)
}
set3 <- rep(NA,41)
for(i in 0:40){
set3[i+1] <-ppois(i,10)
}
plot(set1,pch=19,col="black",ylim=c(0,1),xlim=c(0,21))
points(set2,pch=19,col="skyblue")
points(set3,pch=19,col="gray")
#(2)
cat('min: ', min(co2up), "\n")
cat('max: ', max(co2up), "\n")
cat('median: ', median(co2up), "\n")
cat('mean: ', mean(co2up), "\n")
cat('variance: ', var(co2up), "\n")
cat('standard deviation: ', sd(co2up), "\n", "\n")
cat('Standard Deviation calculated by Jelena:', result$st_dev, "\n", "\n")  #Displays standard deviation
cat('Coefficient of Variation calculated by Jelena:', result$CV, "\n", "\n")  #Displays coefficient of variation
#(10e)
cbind(moment_binom(20,.5),moment_binom(20,.7),moment_binom(40,.5))
cat('Coefficient of Variation calculated by Jelena:', result$CV, "\n", "\n")  #Displays coefficient of variation
mode <- as.numeric(names(m)[m==max(m)]) #gives the names of the table headings with the highest frequency of occurence
mode
q()
## Exercise 2. Model parameter p of Bernoulli distributed data
y <- c(0,0,1,0,1,0,0,0,1,0)
?dbeta
## Exercise 2. Model parameter p of Bernoulli distributed data
y <- c(0,0,1,0,1,0,0,0,1,0)
alpha <- 1
beta <- 1
post_sample <- dbeta(1000,(sum(y) + alpha), (sum(1-y) + beta))
plot(post_sample,type="l")
post_sample
p_vals <- seq(0,1,1000)
post_sample <- dbeta(p_vals,(sum(y) + alpha), (sum(1-y) + beta))
plot(post_sample,type="l")
plot(p_vals,post_sample,type="l")
p_vals
p_vals <- seq(0,1,1000)
p_vals
?seq
p_vals <- seq(0,1,1000,by=.001)
p_vals <- seq(0,1,1000,by=.01)
p_vals <- seq(0,1,.0001)
post_sample <- dbeta(p_vals,(sum(y) + alpha), (sum(1-y) + beta))
plot(p_vals,post_sample,type="l")
## Exercise 2. Model parameter p of Bernoulli distributed data
plot(dbeta(seq(0,1,.0001),1,1),type="l")
dbeta(.2,1,1)
## Exercise 2. Model parameter p of Bernoulli distributed data
plot(seq(0,1,.0001),dbeta(seq(0,1,.0001),1,1),type="l")
y <- c(0,0,1,0,1,0,0,0,1,0)
alpha <- 1
beta <- 1
p_vals <- seq(0,1,.0001)
post_sample <- dbeta(p_vals,(sum(y) + alpha), (sum(1-y) + beta))
plot(p_vals,post_sample,type="l")
?dbeta
?dbeta
prior_sample <- dbeta(p_vals,1,1)
lines(p_vals,prior_sample)
post_sample <- dbeta(p_vals,(sum(y) + alpha), (sum(1-y) + beta))
plot(p_vals,post_sample,type="l")
prior_sample <- dbeta(p_vals,1,1)
lines(p_vals,prior_sample,lty=2)
?lines
post_sample <- dbeta(p_vals,(sum(y) + alpha), (sum(1-y) + beta))
plot(p_vals,post_sample,type="l")
prior_sample <- dbeta(p_vals,1,1)
lines(p_vals,prior_sample)
?dbinom
sum(y)
like_sample <- dbinom(sum(y),length(y),p_vals)
plot(p_vals,like_sample,type="l")
lines(p_vals,post_sample,type="l",col="red")
plot(p_vals,like_sample,type="l")
# Putting it all together
plot(p_vals,post_sample,type="l")
# Putting it all together
plot(p_vals,post_sample,type="l",ylim=c(0,3))
lines(p_vals,prior_sample)
plot(p_vals,like_sample,type="l",col="red")
# Putting it all together
plot(p_vals,post_sample,type="l",ylim=c(0,3))
lines(p_vals,prior_sample)
lines(p_vals,like_sample,type="l",col="red")
max(like_sample)
like_sample[1]
# new version
plot(p_vals,post_sample,type="l",ylim=c(0,3))
lines(p_vals,prior_sample)
lines(p_vals,like_sample/(max(like_sample)),type="l",col="red")
# Change prior
alpha <- 4
beta <- 1
post_sample <- dbeta(p_vals,(sum(y) + alpha), (sum(1-y) + beta))
plot(p_vals,post_sample,type="l",ylim=c(0,3))
lines(p_vals,prior_sample)
lines(p_vals,like_sample/(max(like_sample)),type="l",col="red")
prior_sample <- dbeta(p_vals,1,1)
plot(p_vals,post_sample,type="l",ylim=c(0,3))
lines(p_vals,prior_sample)
prior_sample <- dbeta(p_vals,4,1)
plot(p_vals,post_sample,type="l",ylim=c(0,3))
lines(p_vals,prior_sample)
lines(p_vals,like_sample/(max(like_sample)),type="l",col="red")
?lines
?par
plot(p_vals,post_sample,type="l",ylim=c(0,3))
lines(p_vals,prior_sample,lty=2)
lines(p_vals,like_sample/(max(like_sample)),type="l",col="red")
plot(p_vals,post_sample,type="l",ylim=c(0,3))
lines(p_vals,prior_sample,lty=3)
lines(p_vals,like_sample/(max(like_sample)),type="l",col="red")
# Task 1
y <- c(0,0,1,0,1,0,0,0,1,0)
alpha <- 1
beta <- 1
p_vals <- seq(0,1,.0001)
post_sample <- dbeta(p_vals,(sum(y) + alpha), (sum(1-y) + beta))
plot(p_vals,post_sample,type="l")
post_sample <- dbeta(p_vals,(sum(y) + alpha), (sum(1-y) + beta))
plot(p_vals,post_sample,type="l",xlab="p",ylab="density")
# Task 2
# prior
prior_sample <- dbeta(p_vals,1,1)
lines(p_vals,prior_sample)
lines(p_vals,prior_sample,lty=3)
plot(p_vals,post_sample,type="l",xlab="p",ylab="density")
lines(p_vals,prior_sample,lty=3)
# likelihood function
like_sample <- dbinom(sum(y),length(y),p_vals)
plot(p_vals,like_sample,type="l",col="red")
plot(p_vals,post_sample,type="l",xlab="p",ylab="density")
lines(p_vals,prior_sample,lty=3)
lines(p_vals,like_sample,type="l",col="red")
plot(p_vals,post_sample,type="l",xlab="p",ylab="density")
lines(p_vals,prior_sample,lty=3)
lines(p_vals,like_sample/(max(like_sample)),type="l",col="red")
?legend
legend("topright",c("posterior","prior","likelihood"),col=c("black","black","red"),lty=c(1,3,1))
# Change prior
alpha <- 4
beta <- 1
post_sample <- dbeta(p_vals,(sum(y) + alpha), (sum(1-y) + beta))
prior_sample <- dbeta(p_vals,4,1)
plot(p_vals,post_sample,type="l",xlab="p",ylab="density")
lines(p_vals,prior_sample,lty=3)
lines(p_vals,like_sample/(max(like_sample)),type="l",col="red")
legend("topright",c("posterior","prior","likelihood"),col=c("black","black","red"),lty=c(1,3,1))
plot(p_vals,post_sample,type="l",xlab="p",ylab="density")
lines(p_vals,prior_sample,lty=3)
lines(p_vals,like_sample/(max(like_sample)),type="l",col="red")
legend("topleft",c("posterior","prior","likelihood"),col=c("black","black","red"),lty=c(1,3,1))
rm(list=ls())
# matrixes of visits and repeated visits within the same occasion
# correspond  to two 280*15 matrices (sites by year)
# first eleven rows correspond to visits
# next eleven rows correspond to repeated visits within the same occasion
presence <- matrix(scan("presence.txt", n = 278*30,skip=1), 278, 30, byrow = TRUE)
y <- structure(presence,.Dim = c(278L,15L,2L))
setwd("~/OneDrive/Documents/CNRS/pantel_work/2015_model_run/for_manuscript/1_metapopulation_manuscript/covar/all_sites/results/3_Drepanotrema_depressissimum")
# matrixes of visits and repeated visits within the same occasion
# correspond  to two 280*15 matrices (sites by year)
# first eleven rows correspond to visits
# next eleven rows correspond to repeated visits within the same occasion
presence <- matrix(scan("presence.txt", n = 278*30,skip=1), 278, 30, byrow = TRUE)
y <- structure(presence,.Dim = c(278L,15L,2L))
y[y==0]=NA
y[,,1]
blah <- y[,,1]
View(blah)
####Read in covariates####
# covariates site-specific
cov <- read.table("cov.txt",h=T)
View(cov)
size = cov$size
size <- (size-mean(size))/sd(size)
connec = cov$connec
connec <- (connec-mean(connec))/sd(connec)
veg = cov$veg
veg <- (veg-mean(veg))/sd(veg)
stability = cov$stability
stability <- (stability-mean(stability))/sd(stability)
habit <- read.table("habit.txt",h=T)
mangrove = habit$mangrove
mangrove <- (mangrove-mean(mangrove))/sd(mangrove)
river = habit$river
river <- (river-mean(river))/sd(river)
pluvio <- read.table("pluvio.txt",h=T)
lrs=pluvio$lrs
rs_1=pluvio$rs_July_Dec
lrs <- (lrs-mean(lrs))/sd(lrs)
rs_1 <- (rs_1-mean(rs_1))/sd(rs_1)
# covariates year and site-specific
colsource <- matrix(scan("colsource.txt", n = 278*15), 278, 15, byrow = TRUE)
colsource <- (colsource-mean(colsource))/sqrt(mean((colsource-mean(colsource))*(colsource-mean(colsource))))
# number of sites
nsite<-dim(y)[1]
# number de repetitions
nrep<-2
# number of visits to sites
nyear<-15
blah <- cbind(size,veg,stability,connec,colsource,lrs,rs_1,mangrove,river)
View(blah)
blah <- cbind(size,veg,stability,connec,lrs,rs_1,mangrove,river)
q()
